# BulgakovLM
BulgakovLM is the largest and open neural network for Russian-language, trained on a wide range of data for use in various NLP tasks.

BulgakovLM is in a state of learning! [W&B](https://wandb.ai/0x7o/mesh-transformer/runs/1h27joh5)

## Versions
Name | Parameters (billions) | PPL | Loss
-- | --- | --- | ---
6B| 6.05 | ??? | ???
2.7B | ??? | ??? | ???
1.3B | ??? | ??? | ???
Large | ??? | ??? | ???
Medium | ??? | ??? | ???
Small | ??? | ??? | ???
Tiny | ??? | ??? | ???

## BibTeX entry

To cite this model:
```bibtex
@misc{bulgakovlm-6b,
  author = {0x7o},
  title = {{BulgakovLM-6B: Huge Russian-language model for text generation}},
  howpublished = {\url{https://github.com/0x7o/BulgakovLM}},
  year = 2022,
  month = August
}
```

To cite the codebase that trained this model:
```bibtex
@misc{mesh-transformer-jax,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
```
